{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PlYouPz743AP",
        "outputId": "742b8724-5b85-44da-dbb0-4758ca0e4812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.2240 - loss: 4.1434Epoch 1/200, Time: 67.90s, Loss: 3.7790, Val_Loss: 3.4143, Val_Acc: 0.1818\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 112ms/step - accuracy: 0.2242 - loss: 4.1423 - val_accuracy: 0.1818 - val_loss: 3.4143 - learning_rate: 0.1000\n",
            "Epoch 2/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3438 - loss: 2.9016Epoch 2/200, Time: 39.36s, Loss: 2.7297, Val_Loss: 2.7343, Val_Acc: 0.2432\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 34ms/step - accuracy: 0.3439 - loss: 2.9006 - val_accuracy: 0.2432 - val_loss: 2.7343 - learning_rate: 0.1000\n",
            "Epoch 3/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3600 - loss: 2.3373Epoch 3/200, Time: 12.20s, Loss: 2.2509, Val_Loss: 2.2284, Val_Acc: 0.3302\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.3600 - loss: 2.3366 - val_accuracy: 0.3302 - val_loss: 2.2284 - learning_rate: 0.0999\n",
            "Epoch 4/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3757 - loss: 2.0483Epoch 4/200, Time: 20.50s, Loss: 2.0025, Val_Loss: 2.1365, Val_Acc: 0.2844\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.3758 - loss: 2.0479 - val_accuracy: 0.2844 - val_loss: 2.1365 - learning_rate: 0.0999\n",
            "Epoch 5/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3869 - loss: 1.9006Epoch 5/200, Time: 20.06s, Loss: 1.8746, Val_Loss: 1.8582, Val_Acc: 0.3940\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.3870 - loss: 1.9004 - val_accuracy: 0.3940 - val_loss: 1.8582 - learning_rate: 0.0998\n",
            "Epoch 6/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4009 - loss: 1.8162Epoch 6/200, Time: 21.09s, Loss: 1.8036, Val_Loss: 1.8941, Val_Acc: 0.3560\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.4009 - loss: 1.8162 - val_accuracy: 0.3560 - val_loss: 1.8941 - learning_rate: 0.0998\n",
            "Epoch 7/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4170 - loss: 1.7646Epoch 7/200, Time: 11.81s, Loss: 1.7602, Val_Loss: 1.8661, Val_Acc: 0.3736\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4170 - loss: 1.7645 - val_accuracy: 0.3736 - val_loss: 1.8661 - learning_rate: 0.0997\n",
            "Epoch 8/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4152 - loss: 1.7404Epoch 8/200, Time: 20.58s, Loss: 1.7339, Val_Loss: 1.7371, Val_Acc: 0.4262\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.4153 - loss: 1.7404 - val_accuracy: 0.4262 - val_loss: 1.7371 - learning_rate: 0.0996\n",
            "Epoch 9/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4186 - loss: 1.7256Epoch 9/200, Time: 12.43s, Loss: 1.7150, Val_Loss: 1.7119, Val_Acc: 0.4280\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.4186 - loss: 1.7256 - val_accuracy: 0.4280 - val_loss: 1.7119 - learning_rate: 0.0995\n",
            "Epoch 10/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4379 - loss: 1.6984Epoch 10/200, Time: 12.38s, Loss: 1.6978, Val_Loss: 1.7643, Val_Acc: 0.4140\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4379 - loss: 1.6984 - val_accuracy: 0.4140 - val_loss: 1.7643 - learning_rate: 0.0994\n",
            "Epoch 11/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4438 - loss: 1.6891Epoch 11/200, Time: 12.04s, Loss: 1.6823, Val_Loss: 1.6929, Val_Acc: 0.4370\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.4438 - loss: 1.6891 - val_accuracy: 0.4370 - val_loss: 1.6929 - learning_rate: 0.0993\n",
            "Epoch 12/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4519 - loss: 1.6709Epoch 12/200, Time: 20.30s, Loss: 1.6719, Val_Loss: 1.7109, Val_Acc: 0.4176\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.4519 - loss: 1.6709 - val_accuracy: 0.4176 - val_loss: 1.7109 - learning_rate: 0.0991\n",
            "Epoch 13/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4505 - loss: 1.6634Epoch 13/200, Time: 11.75s, Loss: 1.6649, Val_Loss: 1.8222, Val_Acc: 0.4048\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4505 - loss: 1.6634 - val_accuracy: 0.4048 - val_loss: 1.8222 - learning_rate: 0.0990\n",
            "Epoch 14/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4594 - loss: 1.6566Epoch 14/200, Time: 11.97s, Loss: 1.6512, Val_Loss: 1.7656, Val_Acc: 0.4210\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4594 - loss: 1.6566 - val_accuracy: 0.4210 - val_loss: 1.7656 - learning_rate: 0.0988\n",
            "Epoch 15/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4583 - loss: 1.6506Epoch 15/200, Time: 20.54s, Loss: 1.6482, Val_Loss: 1.6914, Val_Acc: 0.4442\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.4584 - loss: 1.6506 - val_accuracy: 0.4442 - val_loss: 1.6914 - learning_rate: 0.0986\n",
            "Epoch 16/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4616 - loss: 1.6333Epoch 16/200, Time: 20.38s, Loss: 1.6381, Val_Loss: 1.7189, Val_Acc: 0.4226\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.4616 - loss: 1.6333 - val_accuracy: 0.4226 - val_loss: 1.7189 - learning_rate: 0.0984\n",
            "Epoch 17/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4634 - loss: 1.6374Epoch 17/200, Time: 12.19s, Loss: 1.6330, Val_Loss: 1.6963, Val_Acc: 0.4432\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4634 - loss: 1.6374 - val_accuracy: 0.4432 - val_loss: 1.6963 - learning_rate: 0.0982\n",
            "Epoch 18/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4672 - loss: 1.6288Epoch 18/200, Time: 11.83s, Loss: 1.6300, Val_Loss: 1.8133, Val_Acc: 0.4014\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4672 - loss: 1.6288 - val_accuracy: 0.4014 - val_loss: 1.8133 - learning_rate: 0.0980\n",
            "Epoch 19/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4658 - loss: 1.6379Epoch 19/200, Time: 11.91s, Loss: 1.6311, Val_Loss: 1.6829, Val_Acc: 0.4532\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4658 - loss: 1.6379 - val_accuracy: 0.4532 - val_loss: 1.6829 - learning_rate: 0.0978\n",
            "Epoch 20/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4710 - loss: 1.6157Epoch 20/200, Time: 11.99s, Loss: 1.6207, Val_Loss: 1.6491, Val_Acc: 0.4618\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4710 - loss: 1.6158 - val_accuracy: 0.4618 - val_loss: 1.6491 - learning_rate: 0.0975\n",
            "Epoch 21/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4710 - loss: 1.6147Epoch 21/200, Time: 11.79s, Loss: 1.6191, Val_Loss: 1.7145, Val_Acc: 0.4476\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.4710 - loss: 1.6148 - val_accuracy: 0.4476 - val_loss: 1.7145 - learning_rate: 0.0973\n",
            "Epoch 22/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4702 - loss: 1.6183Epoch 22/200, Time: 11.25s, Loss: 1.6174, Val_Loss: 1.6786, Val_Acc: 0.4618\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.4703 - loss: 1.6183 - val_accuracy: 0.4618 - val_loss: 1.6786 - learning_rate: 0.0970\n",
            "Epoch 23/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4769 - loss: 1.6094Epoch 23/200, Time: 11.85s, Loss: 1.6113, Val_Loss: 1.6643, Val_Acc: 0.4658\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4769 - loss: 1.6095 - val_accuracy: 0.4658 - val_loss: 1.6643 - learning_rate: 0.0968\n",
            "Epoch 24/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4765 - loss: 1.6069Epoch 24/200, Time: 20.50s, Loss: 1.6078, Val_Loss: 1.6907, Val_Acc: 0.4506\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.4765 - loss: 1.6069 - val_accuracy: 0.4506 - val_loss: 1.6907 - learning_rate: 0.0965\n",
            "Epoch 25/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4821 - loss: 1.6073Epoch 25/200, Time: 20.81s, Loss: 1.6050, Val_Loss: 1.6917, Val_Acc: 0.4490\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.4821 - loss: 1.6073 - val_accuracy: 0.4490 - val_loss: 1.6917 - learning_rate: 0.0962\n",
            "Epoch 26/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4857 - loss: 1.5834Epoch 26/200, Time: 11.88s, Loss: 1.6006, Val_Loss: 1.6464, Val_Acc: 0.4692\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4856 - loss: 1.5836 - val_accuracy: 0.4692 - val_loss: 1.6464 - learning_rate: 0.0959\n",
            "Epoch 27/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4761 - loss: 1.6025Epoch 27/200, Time: 12.31s, Loss: 1.6014, Val_Loss: 1.6166, Val_Acc: 0.4762\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.4761 - loss: 1.6025 - val_accuracy: 0.4762 - val_loss: 1.6166 - learning_rate: 0.0955\n",
            "Epoch 28/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4824 - loss: 1.5997Epoch 28/200, Time: 20.60s, Loss: 1.5990, Val_Loss: 1.6325, Val_Acc: 0.4756\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.4824 - loss: 1.5997 - val_accuracy: 0.4756 - val_loss: 1.6325 - learning_rate: 0.0952\n",
            "Epoch 29/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4817 - loss: 1.6005Epoch 29/200, Time: 20.48s, Loss: 1.5971, Val_Loss: 1.6990, Val_Acc: 0.4494\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.4817 - loss: 1.6005 - val_accuracy: 0.4494 - val_loss: 1.6990 - learning_rate: 0.0949\n",
            "Epoch 30/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4833 - loss: 1.5965Epoch 30/200, Time: 20.22s, Loss: 1.5933, Val_Loss: 1.6546, Val_Acc: 0.4626\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.4833 - loss: 1.5965 - val_accuracy: 0.4626 - val_loss: 1.6546 - learning_rate: 0.0945\n",
            "Epoch 31/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4849 - loss: 1.5959Epoch 31/200, Time: 11.77s, Loss: 1.5922, Val_Loss: 1.6125, Val_Acc: 0.4842\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4849 - loss: 1.5959 - val_accuracy: 0.4842 - val_loss: 1.6125 - learning_rate: 0.0942\n",
            "Epoch 32/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4914 - loss: 1.5805Epoch 32/200, Time: 20.90s, Loss: 1.5861, Val_Loss: 1.6586, Val_Acc: 0.4620\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.4914 - loss: 1.5805 - val_accuracy: 0.4620 - val_loss: 1.6586 - learning_rate: 0.0938\n",
            "Epoch 33/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4879 - loss: 1.5825Epoch 33/200, Time: 20.24s, Loss: 1.5873, Val_Loss: 1.6542, Val_Acc: 0.4584\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.4879 - loss: 1.5826 - val_accuracy: 0.4584 - val_loss: 1.6542 - learning_rate: 0.0934\n",
            "Epoch 34/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4923 - loss: 1.5791Epoch 34/200, Time: 12.17s, Loss: 1.5846, Val_Loss: 1.6853, Val_Acc: 0.4548\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4923 - loss: 1.5791 - val_accuracy: 0.4548 - val_loss: 1.6853 - learning_rate: 0.0930\n",
            "Epoch 35/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4919 - loss: 1.5758Epoch 35/200, Time: 12.23s, Loss: 1.5856, Val_Loss: 1.6762, Val_Acc: 0.4580\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4919 - loss: 1.5758 - val_accuracy: 0.4580 - val_loss: 1.6762 - learning_rate: 0.0926\n",
            "Epoch 36/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4878 - loss: 1.5864Epoch 36/200, Time: 11.93s, Loss: 1.5840, Val_Loss: 1.6747, Val_Acc: 0.4558\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4878 - loss: 1.5864 - val_accuracy: 0.4558 - val_loss: 1.6747 - learning_rate: 0.0922\n",
            "Epoch 37/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4963 - loss: 1.5754Epoch 37/200, Time: 11.59s, Loss: 1.5766, Val_Loss: 1.6435, Val_Acc: 0.4718\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4963 - loss: 1.5754 - val_accuracy: 0.4718 - val_loss: 1.6435 - learning_rate: 0.0918\n",
            "Epoch 38/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4956 - loss: 1.5719Epoch 38/200, Time: 20.83s, Loss: 1.5763, Val_Loss: 1.5691, Val_Acc: 0.5056\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.4956 - loss: 1.5719 - val_accuracy: 0.5056 - val_loss: 1.5691 - learning_rate: 0.0913\n",
            "Epoch 39/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4970 - loss: 1.5743Epoch 39/200, Time: 20.25s, Loss: 1.5755, Val_Loss: 1.7385, Val_Acc: 0.4496\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.4970 - loss: 1.5743 - val_accuracy: 0.4496 - val_loss: 1.7385 - learning_rate: 0.0909\n",
            "Epoch 40/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4999 - loss: 1.5676Epoch 40/200, Time: 11.74s, Loss: 1.5694, Val_Loss: 1.6519, Val_Acc: 0.4740\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4999 - loss: 1.5676 - val_accuracy: 0.4740 - val_loss: 1.6519 - learning_rate: 0.0904\n",
            "Epoch 41/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5022 - loss: 1.5588Epoch 41/200, Time: 12.30s, Loss: 1.5660, Val_Loss: 1.5892, Val_Acc: 0.4938\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.5022 - loss: 1.5588 - val_accuracy: 0.4938 - val_loss: 1.5892 - learning_rate: 0.0899\n",
            "Epoch 42/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4975 - loss: 1.5714Epoch 42/200, Time: 20.35s, Loss: 1.5689, Val_Loss: 1.6476, Val_Acc: 0.4740\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.4975 - loss: 1.5714 - val_accuracy: 0.4740 - val_loss: 1.6476 - learning_rate: 0.0895\n",
            "Epoch 43/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5011 - loss: 1.5598Epoch 43/200, Time: 20.60s, Loss: 1.5651, Val_Loss: 1.6157, Val_Acc: 0.4784\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.5011 - loss: 1.5599 - val_accuracy: 0.4784 - val_loss: 1.6157 - learning_rate: 0.0890\n",
            "Epoch 44/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5031 - loss: 1.5597Epoch 44/200, Time: 12.34s, Loss: 1.5673, Val_Loss: 1.6175, Val_Acc: 0.4870\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.5031 - loss: 1.5597 - val_accuracy: 0.4870 - val_loss: 1.6175 - learning_rate: 0.0885\n",
            "Epoch 45/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5073 - loss: 1.5520Epoch 45/200, Time: 12.32s, Loss: 1.5598, Val_Loss: 1.6068, Val_Acc: 0.4876\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.5073 - loss: 1.5520 - val_accuracy: 0.4876 - val_loss: 1.6068 - learning_rate: 0.0880\n",
            "Epoch 46/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5117 - loss: 1.5451Epoch 46/200, Time: 11.93s, Loss: 1.5566, Val_Loss: 1.5823, Val_Acc: 0.4922\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5116 - loss: 1.5452 - val_accuracy: 0.4922 - val_loss: 1.5823 - learning_rate: 0.0874\n",
            "Epoch 47/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5094 - loss: 1.5481Epoch 47/200, Time: 11.77s, Loss: 1.5558, Val_Loss: 1.6044, Val_Acc: 0.4886\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5094 - loss: 1.5482 - val_accuracy: 0.4886 - val_loss: 1.6044 - learning_rate: 0.0869\n",
            "Epoch 48/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5122 - loss: 1.5472Epoch 48/200, Time: 11.81s, Loss: 1.5520, Val_Loss: 1.6135, Val_Acc: 0.4960\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c612a3a55b1a>\u001b[0m in \u001b[0;36m<cell line: 224>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mlearning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_schedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearningRateSchedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             ):\n\u001b[0;32m--> 544\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m    545\u001b[0m                     \u001b[0;34m\"This optimizer was created with a `LearningRateSchedule`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0;34m\" object as its `learning_rate` constructor argument, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument."
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "#############################\n",
        "# Configuration and Setup\n",
        "#############################\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "weight_decay = 5e-4\n",
        "initial_lr = 0.1\n",
        "\n",
        "# CIFAR-10 mean/std for normalization\n",
        "cifar10_mean = [0.4914, 0.4822, 0.4465]\n",
        "cifar10_std = [0.2470, 0.2435, 0.2616]\n",
        "\n",
        "#############################\n",
        "# Data Loading and Preprocessing\n",
        "#############################\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "y_train_full = tf.keras.utils.to_categorical(y_train_full, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Split off a validation set\n",
        "val_size = 5000\n",
        "x_val = x_train_full[:val_size]\n",
        "y_val = y_train_full[:val_size]\n",
        "x_train = x_train_full[val_size:]\n",
        "y_train = y_train_full[val_size:]\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_val = x_val.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "def normalize(x):\n",
        "    x = (x - cifar10_mean) / cifar10_std\n",
        "    return x\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomCrop(32, 32),\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\")\n",
        "])\n",
        "\n",
        "#############################\n",
        "# Channel-Wise Learnable Activation\n",
        "#############################\n",
        "class ChannelWiseLearnableActivation(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_units=16, wd=1e-4):\n",
        "        super(ChannelWiseLearnableActivation, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.wd = wd\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        c = input_shape[-1]\n",
        "        self.dense1 = tf.keras.layers.Dense(\n",
        "            self.hidden_units,\n",
        "            activation='relu',\n",
        "            kernel_initializer='he_normal',\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(self.wd)\n",
        "        )\n",
        "        self.dense2 = tf.keras.layers.Dense(\n",
        "            c,\n",
        "            activation='sigmoid',\n",
        "            kernel_initializer='he_normal',\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(self.wd)\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: (batch, h, w, c)\n",
        "        x_mean = tf.reduce_mean(inputs, axis=[1, 2])  # (batch, c)\n",
        "        x_hidden = self.dense1(x_mean)  # (batch, hidden_units)\n",
        "        scale = self.dense2(x_hidden)   # (batch, c)\n",
        "        scale = tf.reshape(scale, [-1, 1, 1, tf.shape(scale)[-1]])\n",
        "        return inputs * scale\n",
        "\n",
        "#############################\n",
        "# ResNet Building Blocks\n",
        "#############################\n",
        "regularizer = tf.keras.regularizers.l2(weight_decay)\n",
        "initializer = tf.keras.initializers.HeNormal()\n",
        "\n",
        "def conv3x3(filters, stride=1):\n",
        "    return tf.keras.layers.Conv2D(filters, kernel_size=3, strides=stride, padding='same',\n",
        "                                  kernel_initializer=initializer,\n",
        "                                  kernel_regularizer=regularizer, use_bias=False)\n",
        "\n",
        "class NoOpLayer(tf.keras.layers.Layer):\n",
        "    def call(self, x, training=None):\n",
        "        return x\n",
        "\n",
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.stride = stride\n",
        "\n",
        "        self.conv1 = conv3x3(filters, stride)\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.act1 = ChannelWiseLearnableActivation(hidden_units=16, wd=weight_decay)\n",
        "\n",
        "        self.conv2 = conv3x3(filters)\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        if stride != 1:\n",
        "            self.shortcut = tf.keras.Sequential([\n",
        "                tf.keras.layers.Conv2D(filters, kernel_size=1, strides=stride,\n",
        "                                       kernel_initializer=initializer,\n",
        "                                       kernel_regularizer=regularizer, use_bias=False),\n",
        "                tf.keras.layers.BatchNormalization()\n",
        "            ])\n",
        "        else:\n",
        "            self.shortcut = NoOpLayer()\n",
        "\n",
        "        self.act2 = ChannelWiseLearnableActivation(hidden_units=16, wd=weight_decay)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        shortcut = self.shortcut(x, training=training)\n",
        "\n",
        "        out = self.conv1(x, training=training)\n",
        "        out = self.bn1(out, training=training)\n",
        "        out = self.act1(out)\n",
        "\n",
        "        out = self.conv2(out, training=training)\n",
        "        out = self.bn2(out, training=training)\n",
        "        out = out + shortcut\n",
        "        out = self.act2(out)\n",
        "        return out\n",
        "\n",
        "def make_layer(filters, num_blocks, stride=1):\n",
        "    layers = []\n",
        "    layers.append(ResidualBlock(filters, stride))\n",
        "    for _ in range(1, num_blocks):\n",
        "        layers.append(ResidualBlock(filters, stride=1))\n",
        "    return tf.keras.Sequential(layers)\n",
        "\n",
        "def build_resnet20():\n",
        "    inputs = tf.keras.Input(shape=(32,32,3))\n",
        "    x = conv3x3(16)(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = ChannelWiseLearnableActivation(hidden_units=16, wd=weight_decay)(x)\n",
        "\n",
        "    # ResNet-20: 3 groups of residual blocks, each with 3 blocks\n",
        "    x = make_layer(16, 3, stride=1)(x)\n",
        "    x = make_layer(32, 3, stride=2)(x)\n",
        "    x = make_layer(64, 3, stride=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    kernel_regularizer=regularizer)(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "#############################\n",
        "# Prepare Datasets\n",
        "#############################\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=50000).batch(batch_size)\n",
        "train_dataset = train_dataset.map(lambda x,y: (data_augmentation(x, training=True), y))\n",
        "train_dataset = train_dataset.map(lambda x,y: (normalize(x), y))\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.map(lambda x,y: (normalize(x), y))\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.map(lambda x,y: (normalize(x), y))\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#############################\n",
        "# Learning Rate Scheduler (Cosine Decay)\n",
        "#############################\n",
        "steps_per_epoch = len(x_train) // batch_size\n",
        "total_steps = steps_per_epoch * epochs\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=initial_lr,\n",
        "    decay_steps=total_steps,\n",
        "    alpha=0.001\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
        "\n",
        "#############################\n",
        "# Training and Callbacks\n",
        "#############################\n",
        "class TimingCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.train_times = []\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        end_time = time.time()\n",
        "        epoch_time = end_time - self.start_time\n",
        "        self.train_times.append(epoch_time)\n",
        "        self.start_time = end_time\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Time: {epoch_time:.2f}s, \"\n",
        "              f\"Loss: {logs.get('loss',0):.4f}, Val_Loss: {logs.get('val_loss',0):.4f}, \"\n",
        "              f\"Val_Acc: {logs.get('val_accuracy',0):.4f}\")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_learn_best_cifar10.keras\", save_best_only=True, monitor='val_loss')\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
        "timing_cb = TimingCallback()\n",
        "\n",
        "#############################\n",
        "# Build and Train Model\n",
        "#############################\n",
        "model = build_resnet20()\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[timing_cb, early_stopping, checkpoint, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(test_dataset, verbose=0)\n",
        "weights = np.concatenate([w.numpy().flatten() for w in model.trainable_variables if w.dtype.is_floating])\n",
        "sparsity = np.mean(np.abs(weights) < 1e-5)\n",
        "total_time = np.sum(timing_cb.train_times)\n",
        "\n",
        "print(\"Learnable Activation ResNet-20 Model:\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Sparsity: {sparsity:.4f}\")\n",
        "print(f\"Total Training Time: {total_time:.2f}s\")\n",
        "\n",
        "# Plot Accuracy Curves\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss Curves\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "#############################\n",
        "# Configuration and Setup\n",
        "#############################\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 200\n",
        "weight_decay = 5e-4\n",
        "initial_lr = 0.1\n",
        "\n",
        "# CIFAR-10 mean/std for normalization\n",
        "cifar10_mean = [0.4914, 0.4822, 0.4465]\n",
        "cifar10_std = [0.2470, 0.2435, 0.2616]\n",
        "\n",
        "#############################\n",
        "# Data Loading and Preprocessing\n",
        "#############################\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "y_train_full = tf.keras.utils.to_categorical(y_train_full, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Split off a validation set\n",
        "val_size = 5000\n",
        "x_val = x_train_full[:val_size]\n",
        "y_val = y_train_full[:val_size]\n",
        "x_train = x_train_full[val_size:]\n",
        "y_train = y_train_full[val_size:]\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_val = x_val.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "def normalize(x):\n",
        "    x = (x - cifar10_mean) / cifar10_std\n",
        "    return x\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomCrop(32, 32),\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\")\n",
        "])\n",
        "\n",
        "#############################\n",
        "# Channel-Wise Learnable Activation\n",
        "#############################\n",
        "class ChannelWiseLearnableActivation(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_units=16, wd=1e-4):\n",
        "        super(ChannelWiseLearnableActivation, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.wd = wd\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        c = input_shape[-1]\n",
        "        self.dense1 = tf.keras.layers.Dense(\n",
        "            self.hidden_units,\n",
        "            activation='relu',\n",
        "            kernel_initializer='he_normal',\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(self.wd)\n",
        "        )\n",
        "        self.dense2 = tf.keras.layers.Dense(\n",
        "            c,\n",
        "            activation='sigmoid',\n",
        "            kernel_initializer='he_normal',\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(self.wd)\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: (batch, h, w, c)\n",
        "        x_mean = tf.reduce_mean(inputs, axis=[1, 2])  # (batch, c)\n",
        "        x_hidden = self.dense1(x_mean)  # (batch, hidden_units)\n",
        "        scale = self.dense2(x_hidden)   # (batch, c)\n",
        "        scale = tf.reshape(scale, [-1, 1, 1, tf.shape(scale)[-1]])\n",
        "        return inputs * scale\n",
        "\n",
        "#############################\n",
        "# ResNet Building Blocks\n",
        "#############################\n",
        "regularizer = tf.keras.regularizers.l2(weight_decay)\n",
        "initializer = tf.keras.initializers.HeNormal()\n",
        "\n",
        "def conv3x3(filters, stride=1):\n",
        "    return tf.keras.layers.Conv2D(filters, kernel_size=3, strides=stride, padding='same',\n",
        "                                  kernel_initializer=initializer,\n",
        "                                  kernel_regularizer=regularizer, use_bias=False)\n",
        "\n",
        "class NoOpLayer(tf.keras.layers.Layer):\n",
        "    def call(self, x, training=None):\n",
        "        return x\n",
        "\n",
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.stride = stride\n",
        "\n",
        "        self.conv1 = conv3x3(filters, stride)\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.act1 = ChannelWiseLearnableActivation(hidden_units=16, wd=weight_decay)\n",
        "\n",
        "        self.conv2 = conv3x3(filters)\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        if stride != 1:\n",
        "            self.shortcut = tf.keras.Sequential([\n",
        "                tf.keras.layers.Conv2D(filters, kernel_size=1, strides=stride,\n",
        "                                       kernel_initializer=initializer,\n",
        "                                       kernel_regularizer=regularizer, use_bias=False),\n",
        "                tf.keras.layers.BatchNormalization()\n",
        "            ])\n",
        "        else:\n",
        "            self.shortcut = NoOpLayer()\n",
        "\n",
        "        self.act2 = ChannelWiseLearnableActivation(hidden_units=16, wd=weight_decay)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        shortcut = self.shortcut(x, training=training)\n",
        "\n",
        "        out = self.conv1(x, training=training)\n",
        "        out = self.bn1(out, training=training)\n",
        "        out = self.act1(out)\n",
        "\n",
        "        out = self.conv2(out, training=training)\n",
        "        out = self.bn2(out, training=training)\n",
        "        out = out + shortcut\n",
        "        out = self.act2(out)\n",
        "        return out\n",
        "\n",
        "def make_layer(filters, num_blocks, stride=1):\n",
        "    layers = []\n",
        "    layers.append(ResidualBlock(filters, stride))\n",
        "    for _ in range(1, num_blocks):\n",
        "        layers.append(ResidualBlock(filters, stride=1))\n",
        "    return tf.keras.Sequential(layers)\n",
        "\n",
        "def build_resnet20():\n",
        "    inputs = tf.keras.Input(shape=(32,32,3))\n",
        "    x = conv3x3(16)(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = ChannelWiseLearnableActivation(hidden_units=16, wd=weight_decay)(x)\n",
        "\n",
        "    # ResNet-20: 3 groups of residual blocks, each with 3 blocks\n",
        "    x = make_layer(16, 3, stride=1)(x)\n",
        "    x = make_layer(32, 3, stride=2)(x)\n",
        "    x = make_layer(64, 3, stride=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    kernel_regularizer=regularizer)(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "#############################\n",
        "# Prepare Datasets\n",
        "#############################\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=50000).batch(batch_size)\n",
        "train_dataset = train_dataset.map(lambda x,y: (data_augmentation(x, training=True), y))\n",
        "train_dataset = train_dataset.map(lambda x,y: (normalize(x), y))\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.map(lambda x,y: (normalize(x), y))\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.map(lambda x,y: (normalize(x), y))\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#############################\n",
        "# Learning Rate Scheduler (Cosine Decay)\n",
        "#############################\n",
        "steps_per_epoch = len(x_train) // batch_size\n",
        "total_steps = steps_per_epoch * epochs\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=initial_lr,\n",
        "    decay_steps=total_steps,\n",
        "    alpha=0.001\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
        "\n",
        "#############################\n",
        "# Training and Callbacks\n",
        "#############################\n",
        "class TimingCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.train_times = []\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        end_time = time.time()\n",
        "        epoch_time = end_time - self.start_time\n",
        "        self.train_times.append(epoch_time)\n",
        "        self.start_time = end_time\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Time: {epoch_time:.2f}s, \"\n",
        "              f\"Loss: {logs.get('loss',0):.4f}, Val_Loss: {logs.get('val_loss',0):.4f}, \"\n",
        "              f\"Val_Acc: {logs.get('val_accuracy',0):.4f}\")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_learn_best_cifar10.keras\", save_best_only=True, monitor='val_loss')\n",
        "timing_cb = TimingCallback()\n",
        "\n",
        "#############################\n",
        "# Build and Train Model\n",
        "#############################\n",
        "model = build_resnet20()\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[timing_cb, early_stopping, checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(test_dataset, verbose=0)\n",
        "weights = np.concatenate([w.numpy().flatten() for w in model.trainable_variables if w.dtype.is_floating])\n",
        "sparsity = np.mean(np.abs(weights) < 1e-5)\n",
        "total_time = np.sum(timing_cb.train_times)\n",
        "\n",
        "print(\"Learnable Activation ResNet-20 Model:\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Sparsity: {sparsity:.4f}\")\n",
        "print(f\"Total Training Time: {total_time:.2f}s\")\n",
        "\n",
        "# Plot Accuracy Curves\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss Curves\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TJDxlb4U-9OS",
        "outputId": "013c7f0b-14f7-4bb5-8b36-510cae00ca86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Epoch 1/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.2117 - loss: 4.3689Epoch 1/200, Time: 64.12s, Loss: 4.0342, Val_Loss: 3.5841, Val_Acc: 0.1706\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 101ms/step - accuracy: 0.2118 - loss: 4.3680 - val_accuracy: 0.1706 - val_loss: 3.5841\n",
            "Epoch 2/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3220 - loss: 3.0927Epoch 2/200, Time: 11.68s, Loss: 2.8940, Val_Loss: 2.8141, Val_Acc: 0.2340\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.3220 - loss: 3.0922 - val_accuracy: 0.2340 - val_loss: 2.8141\n",
            "Epoch 3/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3468 - loss: 2.4387Epoch 3/200, Time: 12.21s, Loss: 2.3398, Val_Loss: 2.3317, Val_Acc: 0.2930\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.3468 - loss: 2.4382 - val_accuracy: 0.2930 - val_loss: 2.3317\n",
            "Epoch 4/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3596 - loss: 2.1183Epoch 4/200, Time: 11.46s, Loss: 2.0626, Val_Loss: 2.1299, Val_Acc: 0.2928\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.3597 - loss: 2.1180 - val_accuracy: 0.2928 - val_loss: 2.1299\n",
            "Epoch 5/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3735 - loss: 1.9506Epoch 5/200, Time: 11.46s, Loss: 1.9202, Val_Loss: 1.9494, Val_Acc: 0.3622\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.3735 - loss: 1.9505 - val_accuracy: 0.3622 - val_loss: 1.9494\n",
            "Epoch 6/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3897 - loss: 1.8491Epoch 6/200, Time: 20.37s, Loss: 1.8355, Val_Loss: 1.8895, Val_Acc: 0.3534\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.3897 - loss: 1.8490 - val_accuracy: 0.3534 - val_loss: 1.8895\n",
            "Epoch 7/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4059 - loss: 1.7855Epoch 7/200, Time: 11.20s, Loss: 1.7782, Val_Loss: 1.8184, Val_Acc: 0.3914\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.4059 - loss: 1.7855 - val_accuracy: 0.3914 - val_loss: 1.8184\n",
            "Epoch 8/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4124 - loss: 1.7519Epoch 8/200, Time: 21.12s, Loss: 1.7450, Val_Loss: 1.7340, Val_Acc: 0.4154\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.4124 - loss: 1.7518 - val_accuracy: 0.4154 - val_loss: 1.7340\n",
            "Epoch 9/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4202 - loss: 1.7274Epoch 9/200, Time: 11.91s, Loss: 1.7175, Val_Loss: 1.7179, Val_Acc: 0.4226\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4202 - loss: 1.7273 - val_accuracy: 0.4226 - val_loss: 1.7179\n",
            "Epoch 10/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4334 - loss: 1.7011Epoch 10/200, Time: 11.65s, Loss: 1.7036, Val_Loss: 1.7679, Val_Acc: 0.3990\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.4334 - loss: 1.7011 - val_accuracy: 0.3990 - val_loss: 1.7679\n",
            "Epoch 11/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4354 - loss: 1.6972Epoch 11/200, Time: 11.36s, Loss: 1.6857, Val_Loss: 1.7879, Val_Acc: 0.4070\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.4354 - loss: 1.6971 - val_accuracy: 0.4070 - val_loss: 1.7879\n",
            "Epoch 12/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4456 - loss: 1.6752Epoch 12/200, Time: 20.41s, Loss: 1.6759, Val_Loss: 1.7157, Val_Acc: 0.4326\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.4455 - loss: 1.6752 - val_accuracy: 0.4326 - val_loss: 1.7157\n",
            "Epoch 13/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4475 - loss: 1.6656Epoch 13/200, Time: 20.98s, Loss: 1.6675, Val_Loss: 1.6848, Val_Acc: 0.4390\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.4475 - loss: 1.6656 - val_accuracy: 0.4390 - val_loss: 1.6848\n",
            "Epoch 14/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4524 - loss: 1.6567Epoch 14/200, Time: 20.20s, Loss: 1.6529, Val_Loss: 1.6809, Val_Acc: 0.4312\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.4524 - loss: 1.6566 - val_accuracy: 0.4312 - val_loss: 1.6809\n",
            "Epoch 15/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4526 - loss: 1.6508Epoch 15/200, Time: 12.13s, Loss: 1.6530, Val_Loss: 1.7184, Val_Acc: 0.4280\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4526 - loss: 1.6508 - val_accuracy: 0.4280 - val_loss: 1.7184\n",
            "Epoch 16/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4589 - loss: 1.6361Epoch 16/200, Time: 19.84s, Loss: 1.6433, Val_Loss: 1.7594, Val_Acc: 0.4088\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.4589 - loss: 1.6361 - val_accuracy: 0.4088 - val_loss: 1.7594\n",
            "Epoch 17/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4560 - loss: 1.6419Epoch 17/200, Time: 11.47s, Loss: 1.6363, Val_Loss: 1.7287, Val_Acc: 0.4262\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.4560 - loss: 1.6419 - val_accuracy: 0.4262 - val_loss: 1.7287\n",
            "Epoch 18/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4614 - loss: 1.6344Epoch 18/200, Time: 11.56s, Loss: 1.6355, Val_Loss: 1.8532, Val_Acc: 0.3806\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4614 - loss: 1.6344 - val_accuracy: 0.3806 - val_loss: 1.8532\n",
            "Epoch 19/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4603 - loss: 1.6418Epoch 19/200, Time: 20.41s, Loss: 1.6348, Val_Loss: 1.6614, Val_Acc: 0.4558\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.4603 - loss: 1.6417 - val_accuracy: 0.4558 - val_loss: 1.6614\n",
            "Epoch 20/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4645 - loss: 1.6261Epoch 20/200, Time: 11.84s, Loss: 1.6281, Val_Loss: 1.7358, Val_Acc: 0.4238\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4645 - loss: 1.6261 - val_accuracy: 0.4238 - val_loss: 1.7358\n",
            "Epoch 21/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4669 - loss: 1.6238Epoch 21/200, Time: 20.42s, Loss: 1.6275, Val_Loss: 1.6716, Val_Acc: 0.4538\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.4669 - loss: 1.6238 - val_accuracy: 0.4538 - val_loss: 1.6716\n",
            "Epoch 22/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4675 - loss: 1.6199Epoch 22/200, Time: 11.58s, Loss: 1.6204, Val_Loss: 1.6697, Val_Acc: 0.4450\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4675 - loss: 1.6199 - val_accuracy: 0.4450 - val_loss: 1.6697\n",
            "Epoch 23/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4697 - loss: 1.6155Epoch 23/200, Time: 11.54s, Loss: 1.6166, Val_Loss: 1.7324, Val_Acc: 0.4310\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4697 - loss: 1.6155 - val_accuracy: 0.4310 - val_loss: 1.7324\n",
            "Epoch 24/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4705 - loss: 1.6115Epoch 24/200, Time: 20.43s, Loss: 1.6140, Val_Loss: 1.7206, Val_Acc: 0.4488\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.4705 - loss: 1.6115 - val_accuracy: 0.4488 - val_loss: 1.7206\n",
            "Epoch 25/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4749 - loss: 1.6132Epoch 25/200, Time: 11.37s, Loss: 1.6118, Val_Loss: 1.6449, Val_Acc: 0.4612\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4749 - loss: 1.6132 - val_accuracy: 0.4612 - val_loss: 1.6449\n",
            "Epoch 26/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4830 - loss: 1.5944Epoch 26/200, Time: 20.97s, Loss: 1.6093, Val_Loss: 1.6962, Val_Acc: 0.4452\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.4829 - loss: 1.5946 - val_accuracy: 0.4452 - val_loss: 1.6962\n",
            "Epoch 27/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4731 - loss: 1.6110Epoch 27/200, Time: 11.84s, Loss: 1.6069, Val_Loss: 1.6518, Val_Acc: 0.4670\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4732 - loss: 1.6109 - val_accuracy: 0.4670 - val_loss: 1.6518\n",
            "Epoch 28/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4789 - loss: 1.5996Epoch 28/200, Time: 11.51s, Loss: 1.6035, Val_Loss: 1.6460, Val_Acc: 0.4562\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.4789 - loss: 1.5996 - val_accuracy: 0.4562 - val_loss: 1.6460\n",
            "Epoch 29/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4790 - loss: 1.5974Epoch 29/200, Time: 11.78s, Loss: 1.5989, Val_Loss: 1.6387, Val_Acc: 0.4708\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4790 - loss: 1.5974 - val_accuracy: 0.4708 - val_loss: 1.6387\n",
            "Epoch 30/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4783 - loss: 1.5994Epoch 30/200, Time: 20.20s, Loss: 1.5991, Val_Loss: 1.8261, Val_Acc: 0.4280\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.4783 - loss: 1.5994 - val_accuracy: 0.4280 - val_loss: 1.8261\n",
            "Epoch 31/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4813 - loss: 1.5979Epoch 31/200, Time: 20.13s, Loss: 1.5969, Val_Loss: 1.6396, Val_Acc: 0.4732\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.4814 - loss: 1.5979 - val_accuracy: 0.4732 - val_loss: 1.6396\n",
            "Epoch 32/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4867 - loss: 1.5865Epoch 32/200, Time: 21.12s, Loss: 1.5925, Val_Loss: 1.6972, Val_Acc: 0.4426\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.4867 - loss: 1.5865 - val_accuracy: 0.4426 - val_loss: 1.6972\n",
            "Epoch 33/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4812 - loss: 1.5880Epoch 33/200, Time: 11.59s, Loss: 1.5947, Val_Loss: 1.6947, Val_Acc: 0.4502\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4811 - loss: 1.5880 - val_accuracy: 0.4502 - val_loss: 1.6947\n",
            "Epoch 34/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4830 - loss: 1.5870Epoch 34/200, Time: 20.73s, Loss: 1.5906, Val_Loss: 1.7075, Val_Acc: 0.4550\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.4830 - loss: 1.5870 - val_accuracy: 0.4550 - val_loss: 1.7075\n",
            "Epoch 35/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4888 - loss: 1.5780Epoch 35/200, Time: 20.18s, Loss: 1.5903, Val_Loss: 1.6348, Val_Acc: 0.4708\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.4888 - loss: 1.5781 - val_accuracy: 0.4708 - val_loss: 1.6348\n",
            "Epoch 36/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4851 - loss: 1.5874Epoch 36/200, Time: 12.19s, Loss: 1.5856, Val_Loss: 1.6253, Val_Acc: 0.4740\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4851 - loss: 1.5874 - val_accuracy: 0.4740 - val_loss: 1.6253\n",
            "Epoch 37/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4925 - loss: 1.5821Epoch 37/200, Time: 12.22s, Loss: 1.5830, Val_Loss: 1.6198, Val_Acc: 0.4758\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.4925 - loss: 1.5822 - val_accuracy: 0.4758 - val_loss: 1.6198\n",
            "Epoch 38/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4917 - loss: 1.5741Epoch 38/200, Time: 20.40s, Loss: 1.5810, Val_Loss: 1.6446, Val_Acc: 0.4776\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.4917 - loss: 1.5742 - val_accuracy: 0.4776 - val_loss: 1.6446\n",
            "Epoch 39/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4869 - loss: 1.5863Epoch 39/200, Time: 11.47s, Loss: 1.5869, Val_Loss: 1.8042, Val_Acc: 0.4282\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.4869 - loss: 1.5863 - val_accuracy: 0.4282 - val_loss: 1.8042\n",
            "Epoch 40/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4939 - loss: 1.5762Epoch 40/200, Time: 20.30s, Loss: 1.5774, Val_Loss: 1.6473, Val_Acc: 0.4738\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.4939 - loss: 1.5762 - val_accuracy: 0.4738 - val_loss: 1.6473\n",
            "Epoch 41/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4948 - loss: 1.5713Epoch 41/200, Time: 20.83s, Loss: 1.5744, Val_Loss: 1.6785, Val_Acc: 0.4594\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.4948 - loss: 1.5713 - val_accuracy: 0.4594 - val_loss: 1.6785\n",
            "Epoch 42/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4940 - loss: 1.5769Epoch 42/200, Time: 20.50s, Loss: 1.5772, Val_Loss: 1.6877, Val_Acc: 0.4628\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.4940 - loss: 1.5769 - val_accuracy: 0.4628 - val_loss: 1.6877\n",
            "Epoch 43/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4953 - loss: 1.5663Epoch 43/200, Time: 20.15s, Loss: 1.5730, Val_Loss: 1.6350, Val_Acc: 0.4786\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.4953 - loss: 1.5664 - val_accuracy: 0.4786 - val_loss: 1.6350\n",
            "Epoch 44/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4930 - loss: 1.5686Epoch 44/200, Time: 11.46s, Loss: 1.5744, Val_Loss: 1.6091, Val_Acc: 0.4848\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.4930 - loss: 1.5686 - val_accuracy: 0.4848 - val_loss: 1.6091\n",
            "Epoch 45/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4991 - loss: 1.5614Epoch 45/200, Time: 11.79s, Loss: 1.5700, Val_Loss: 1.6617, Val_Acc: 0.4658\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.4991 - loss: 1.5615 - val_accuracy: 0.4658 - val_loss: 1.6617\n",
            "Epoch 46/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5006 - loss: 1.5569Epoch 46/200, Time: 20.64s, Loss: 1.5689, Val_Loss: 1.6083, Val_Acc: 0.4828\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5006 - loss: 1.5570 - val_accuracy: 0.4828 - val_loss: 1.6083\n",
            "Epoch 47/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4994 - loss: 1.5517Epoch 47/200, Time: 20.50s, Loss: 1.5646, Val_Loss: 1.6285, Val_Acc: 0.4846\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.4994 - loss: 1.5518 - val_accuracy: 0.4846 - val_loss: 1.6285\n",
            "Epoch 48/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5007 - loss: 1.5545Epoch 48/200, Time: 20.41s, Loss: 1.5626, Val_Loss: 1.6436, Val_Acc: 0.4670\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5007 - loss: 1.5545 - val_accuracy: 0.4670 - val_loss: 1.6436\n",
            "Epoch 49/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4969 - loss: 1.5690Epoch 49/200, Time: 20.58s, Loss: 1.5644, Val_Loss: 1.6717, Val_Acc: 0.4662\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.4970 - loss: 1.5690 - val_accuracy: 0.4662 - val_loss: 1.6717\n",
            "Epoch 50/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5020 - loss: 1.5561Epoch 50/200, Time: 20.38s, Loss: 1.5600, Val_Loss: 1.5897, Val_Acc: 0.4946\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5020 - loss: 1.5562 - val_accuracy: 0.4946 - val_loss: 1.5897\n",
            "Epoch 51/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4999 - loss: 1.5548Epoch 51/200, Time: 11.78s, Loss: 1.5598, Val_Loss: 1.6654, Val_Acc: 0.4736\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.4999 - loss: 1.5548 - val_accuracy: 0.4736 - val_loss: 1.6654\n",
            "Epoch 52/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5003 - loss: 1.5536Epoch 52/200, Time: 20.96s, Loss: 1.5613, Val_Loss: 1.5895, Val_Acc: 0.4934\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5003 - loss: 1.5536 - val_accuracy: 0.4934 - val_loss: 1.5895\n",
            "Epoch 53/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5051 - loss: 1.5541Epoch 53/200, Time: 20.15s, Loss: 1.5548, Val_Loss: 1.6650, Val_Acc: 0.4828\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5050 - loss: 1.5541 - val_accuracy: 0.4828 - val_loss: 1.6650\n",
            "Epoch 54/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5014 - loss: 1.5520Epoch 54/200, Time: 20.56s, Loss: 1.5514, Val_Loss: 1.6206, Val_Acc: 0.4800\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5014 - loss: 1.5520 - val_accuracy: 0.4800 - val_loss: 1.6206\n",
            "Epoch 55/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5054 - loss: 1.5489Epoch 55/200, Time: 11.40s, Loss: 1.5523, Val_Loss: 1.6181, Val_Acc: 0.4870\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.5054 - loss: 1.5489 - val_accuracy: 0.4870 - val_loss: 1.6181\n",
            "Epoch 56/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5043 - loss: 1.5516Epoch 56/200, Time: 11.25s, Loss: 1.5510, Val_Loss: 1.7424, Val_Acc: 0.4498\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.5043 - loss: 1.5515 - val_accuracy: 0.4498 - val_loss: 1.7424\n",
            "Epoch 57/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5063 - loss: 1.5403Epoch 57/200, Time: 20.74s, Loss: 1.5487, Val_Loss: 1.5886, Val_Acc: 0.4936\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5063 - loss: 1.5403 - val_accuracy: 0.4936 - val_loss: 1.5886\n",
            "Epoch 58/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5101 - loss: 1.5372Epoch 58/200, Time: 11.79s, Loss: 1.5439, Val_Loss: 1.5568, Val_Acc: 0.5076\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5101 - loss: 1.5372 - val_accuracy: 0.5076 - val_loss: 1.5568\n",
            "Epoch 59/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5049 - loss: 1.5443Epoch 59/200, Time: 11.87s, Loss: 1.5434, Val_Loss: 1.6002, Val_Acc: 0.4846\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5050 - loss: 1.5443 - val_accuracy: 0.4846 - val_loss: 1.6002\n",
            "Epoch 60/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5066 - loss: 1.5358Epoch 60/200, Time: 20.51s, Loss: 1.5403, Val_Loss: 1.7741, Val_Acc: 0.4340\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5066 - loss: 1.5358 - val_accuracy: 0.4340 - val_loss: 1.7741\n",
            "Epoch 61/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5119 - loss: 1.5392Epoch 61/200, Time: 11.35s, Loss: 1.5411, Val_Loss: 1.6178, Val_Acc: 0.4842\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.5119 - loss: 1.5392 - val_accuracy: 0.4842 - val_loss: 1.6178\n",
            "Epoch 62/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5084 - loss: 1.5348Epoch 62/200, Time: 20.93s, Loss: 1.5385, Val_Loss: 1.6614, Val_Acc: 0.4868\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5084 - loss: 1.5348 - val_accuracy: 0.4868 - val_loss: 1.6614\n",
            "Epoch 63/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5114 - loss: 1.5367Epoch 63/200, Time: 20.48s, Loss: 1.5385, Val_Loss: 1.5708, Val_Acc: 0.5032\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5114 - loss: 1.5367 - val_accuracy: 0.5032 - val_loss: 1.5708\n",
            "Epoch 64/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5067 - loss: 1.5351Epoch 64/200, Time: 20.42s, Loss: 1.5355, Val_Loss: 1.5790, Val_Acc: 0.4996\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5067 - loss: 1.5351 - val_accuracy: 0.4996 - val_loss: 1.5790\n",
            "Epoch 65/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5171 - loss: 1.5153Epoch 65/200, Time: 20.44s, Loss: 1.5337, Val_Loss: 1.6012, Val_Acc: 0.5014\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5171 - loss: 1.5155 - val_accuracy: 0.5014 - val_loss: 1.6012\n",
            "Epoch 66/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5145 - loss: 1.5268Epoch 66/200, Time: 11.66s, Loss: 1.5324, Val_Loss: 1.6571, Val_Acc: 0.4752\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5145 - loss: 1.5268 - val_accuracy: 0.4752 - val_loss: 1.6571\n",
            "Epoch 67/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5123 - loss: 1.5241Epoch 67/200, Time: 20.34s, Loss: 1.5338, Val_Loss: 1.5786, Val_Acc: 0.4958\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.5123 - loss: 1.5242 - val_accuracy: 0.4958 - val_loss: 1.5786\n",
            "Epoch 68/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5183 - loss: 1.5169Epoch 68/200, Time: 20.53s, Loss: 1.5266, Val_Loss: 1.7321, Val_Acc: 0.4500\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 32ms/step - accuracy: 0.5183 - loss: 1.5170 - val_accuracy: 0.4500 - val_loss: 1.7321\n",
            "Epoch 69/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5208 - loss: 1.5122Epoch 69/200, Time: 20.58s, Loss: 1.5269, Val_Loss: 1.6414, Val_Acc: 0.4820\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5208 - loss: 1.5123 - val_accuracy: 0.4820 - val_loss: 1.6414\n",
            "Epoch 70/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5194 - loss: 1.5168Epoch 70/200, Time: 11.70s, Loss: 1.5246, Val_Loss: 1.6338, Val_Acc: 0.4808\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5194 - loss: 1.5168 - val_accuracy: 0.4808 - val_loss: 1.6338\n",
            "Epoch 71/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5184 - loss: 1.5121Epoch 71/200, Time: 20.44s, Loss: 1.5207, Val_Loss: 1.5740, Val_Acc: 0.5054\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5184 - loss: 1.5122 - val_accuracy: 0.5054 - val_loss: 1.5740\n",
            "Epoch 72/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5240 - loss: 1.5125Epoch 72/200, Time: 20.80s, Loss: 1.5228, Val_Loss: 1.5706, Val_Acc: 0.5038\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5239 - loss: 1.5126 - val_accuracy: 0.5038 - val_loss: 1.5706\n",
            "Epoch 73/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5142 - loss: 1.5224Epoch 73/200, Time: 11.64s, Loss: 1.5174, Val_Loss: 1.5413, Val_Acc: 0.5180\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5142 - loss: 1.5224 - val_accuracy: 0.5180 - val_loss: 1.5413\n",
            "Epoch 74/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5162 - loss: 1.5122Epoch 74/200, Time: 12.35s, Loss: 1.5175, Val_Loss: 1.5894, Val_Acc: 0.4952\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5162 - loss: 1.5122 - val_accuracy: 0.4952 - val_loss: 1.5894\n",
            "Epoch 75/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5237 - loss: 1.5107Epoch 75/200, Time: 20.03s, Loss: 1.5156, Val_Loss: 1.5868, Val_Acc: 0.4950\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5237 - loss: 1.5107 - val_accuracy: 0.4950 - val_loss: 1.5868\n",
            "Epoch 76/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5251 - loss: 1.5040Epoch 76/200, Time: 11.83s, Loss: 1.5122, Val_Loss: 1.5785, Val_Acc: 0.4974\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5251 - loss: 1.5041 - val_accuracy: 0.4974 - val_loss: 1.5785\n",
            "Epoch 77/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5263 - loss: 1.4973Epoch 77/200, Time: 20.39s, Loss: 1.5097, Val_Loss: 1.6100, Val_Acc: 0.5014\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5262 - loss: 1.4974 - val_accuracy: 0.5014 - val_loss: 1.6100\n",
            "Epoch 78/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5249 - loss: 1.4983Epoch 78/200, Time: 11.73s, Loss: 1.5082, Val_Loss: 1.6075, Val_Acc: 0.4840\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5249 - loss: 1.4983 - val_accuracy: 0.4840 - val_loss: 1.6075\n",
            "Epoch 79/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5263 - loss: 1.4866Epoch 79/200, Time: 11.97s, Loss: 1.5073, Val_Loss: 1.6094, Val_Acc: 0.4904\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5262 - loss: 1.4867 - val_accuracy: 0.4904 - val_loss: 1.6094\n",
            "Epoch 80/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5225 - loss: 1.4974Epoch 80/200, Time: 11.62s, Loss: 1.5022, Val_Loss: 1.6792, Val_Acc: 0.4812\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5225 - loss: 1.4974 - val_accuracy: 0.4812 - val_loss: 1.6792\n",
            "Epoch 81/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5255 - loss: 1.4941Epoch 81/200, Time: 11.47s, Loss: 1.5016, Val_Loss: 1.5479, Val_Acc: 0.5076\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.5255 - loss: 1.4941 - val_accuracy: 0.5076 - val_loss: 1.5479\n",
            "Epoch 82/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5247 - loss: 1.4941Epoch 82/200, Time: 11.70s, Loss: 1.4983, Val_Loss: 1.5636, Val_Acc: 0.5074\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5247 - loss: 1.4941 - val_accuracy: 0.5074 - val_loss: 1.5636\n",
            "Epoch 83/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5319 - loss: 1.4889Epoch 83/200, Time: 11.86s, Loss: 1.4982, Val_Loss: 1.6302, Val_Acc: 0.4842\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5319 - loss: 1.4890 - val_accuracy: 0.4842 - val_loss: 1.6302\n",
            "Epoch 84/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5294 - loss: 1.4905Epoch 84/200, Time: 11.53s, Loss: 1.4949, Val_Loss: 1.5607, Val_Acc: 0.5182\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.5294 - loss: 1.4905 - val_accuracy: 0.5182 - val_loss: 1.5607\n",
            "Epoch 85/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5258 - loss: 1.4948Epoch 85/200, Time: 11.69s, Loss: 1.4926, Val_Loss: 1.5677, Val_Acc: 0.5140\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5258 - loss: 1.4948 - val_accuracy: 0.5140 - val_loss: 1.5677\n",
            "Epoch 86/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5314 - loss: 1.4754Epoch 86/200, Time: 11.64s, Loss: 1.4880, Val_Loss: 1.5292, Val_Acc: 0.5126\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5314 - loss: 1.4755 - val_accuracy: 0.5126 - val_loss: 1.5292\n",
            "Epoch 87/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5327 - loss: 1.4808Epoch 87/200, Time: 12.06s, Loss: 1.4893, Val_Loss: 1.5967, Val_Acc: 0.4842\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5326 - loss: 1.4809 - val_accuracy: 0.4842 - val_loss: 1.5967\n",
            "Epoch 88/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5340 - loss: 1.4759Epoch 88/200, Time: 11.68s, Loss: 1.4851, Val_Loss: 1.5307, Val_Acc: 0.5104\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5339 - loss: 1.4760 - val_accuracy: 0.5104 - val_loss: 1.5307\n",
            "Epoch 89/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5323 - loss: 1.4786Epoch 89/200, Time: 20.39s, Loss: 1.4873, Val_Loss: 1.5746, Val_Acc: 0.5034\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5323 - loss: 1.4787 - val_accuracy: 0.5034 - val_loss: 1.5746\n",
            "Epoch 90/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5347 - loss: 1.4762Epoch 90/200, Time: 11.97s, Loss: 1.4814, Val_Loss: 1.5425, Val_Acc: 0.5174\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5347 - loss: 1.4762 - val_accuracy: 0.5174 - val_loss: 1.5425\n",
            "Epoch 91/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5339 - loss: 1.4759Epoch 91/200, Time: 11.73s, Loss: 1.4783, Val_Loss: 1.6207, Val_Acc: 0.4808\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5339 - loss: 1.4759 - val_accuracy: 0.4808 - val_loss: 1.6207\n",
            "Epoch 92/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5354 - loss: 1.4704Epoch 92/200, Time: 20.43s, Loss: 1.4775, Val_Loss: 1.5308, Val_Acc: 0.5138\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5354 - loss: 1.4704 - val_accuracy: 0.5138 - val_loss: 1.5308\n",
            "Epoch 93/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5350 - loss: 1.4689Epoch 93/200, Time: 20.59s, Loss: 1.4730, Val_Loss: 1.5183, Val_Acc: 0.5220\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5350 - loss: 1.4689 - val_accuracy: 0.5220 - val_loss: 1.5183\n",
            "Epoch 94/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5346 - loss: 1.4671Epoch 94/200, Time: 12.13s, Loss: 1.4719, Val_Loss: 1.5368, Val_Acc: 0.5174\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5346 - loss: 1.4671 - val_accuracy: 0.5174 - val_loss: 1.5368\n",
            "Epoch 95/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5380 - loss: 1.4603Epoch 95/200, Time: 20.42s, Loss: 1.4704, Val_Loss: 1.5239, Val_Acc: 0.5168\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5380 - loss: 1.4603 - val_accuracy: 0.5168 - val_loss: 1.5239\n",
            "Epoch 96/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5372 - loss: 1.4589Epoch 96/200, Time: 11.71s, Loss: 1.4681, Val_Loss: 1.6787, Val_Acc: 0.4688\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5372 - loss: 1.4590 - val_accuracy: 0.4688 - val_loss: 1.6787\n",
            "Epoch 97/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5392 - loss: 1.4580Epoch 97/200, Time: 20.71s, Loss: 1.4666, Val_Loss: 1.6008, Val_Acc: 0.4962\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5392 - loss: 1.4581 - val_accuracy: 0.4962 - val_loss: 1.6008\n",
            "Epoch 98/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5373 - loss: 1.4563Epoch 98/200, Time: 11.62s, Loss: 1.4650, Val_Loss: 1.5545, Val_Acc: 0.5134\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5373 - loss: 1.4564 - val_accuracy: 0.5134 - val_loss: 1.5545\n",
            "Epoch 99/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5380 - loss: 1.4604Epoch 99/200, Time: 12.06s, Loss: 1.4616, Val_Loss: 1.5934, Val_Acc: 0.4950\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5380 - loss: 1.4604 - val_accuracy: 0.4950 - val_loss: 1.5934\n",
            "Epoch 100/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5448 - loss: 1.4473Epoch 100/200, Time: 20.18s, Loss: 1.4578, Val_Loss: 1.5870, Val_Acc: 0.4988\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5448 - loss: 1.4474 - val_accuracy: 0.4988 - val_loss: 1.5870\n",
            "Epoch 101/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5438 - loss: 1.4479Epoch 101/200, Time: 11.74s, Loss: 1.4519, Val_Loss: 1.5355, Val_Acc: 0.5194\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5438 - loss: 1.4480 - val_accuracy: 0.5194 - val_loss: 1.5355\n",
            "Epoch 102/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5431 - loss: 1.4498Epoch 102/200, Time: 11.79s, Loss: 1.4553, Val_Loss: 1.5238, Val_Acc: 0.5262\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5431 - loss: 1.4499 - val_accuracy: 0.5262 - val_loss: 1.5238\n",
            "Epoch 103/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5441 - loss: 1.4401Epoch 103/200, Time: 20.65s, Loss: 1.4529, Val_Loss: 1.5171, Val_Acc: 0.5314\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.5441 - loss: 1.4402 - val_accuracy: 0.5314 - val_loss: 1.5171\n",
            "Epoch 104/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5482 - loss: 1.4412Epoch 104/200, Time: 20.60s, Loss: 1.4488, Val_Loss: 1.5808, Val_Acc: 0.4990\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.5482 - loss: 1.4412 - val_accuracy: 0.4990 - val_loss: 1.5808\n",
            "Epoch 105/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5475 - loss: 1.4350Epoch 105/200, Time: 20.03s, Loss: 1.4405, Val_Loss: 1.4858, Val_Acc: 0.5364\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.5475 - loss: 1.4350 - val_accuracy: 0.5364 - val_loss: 1.4858\n",
            "Epoch 106/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5464 - loss: 1.4326Epoch 106/200, Time: 20.59s, Loss: 1.4416, Val_Loss: 1.4975, Val_Acc: 0.5302\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5464 - loss: 1.4326 - val_accuracy: 0.5302 - val_loss: 1.4975\n",
            "Epoch 107/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5474 - loss: 1.4312Epoch 107/200, Time: 12.03s, Loss: 1.4383, Val_Loss: 1.5241, Val_Acc: 0.5184\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5474 - loss: 1.4312 - val_accuracy: 0.5184 - val_loss: 1.5241\n",
            "Epoch 108/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5527 - loss: 1.4273Epoch 108/200, Time: 11.74s, Loss: 1.4369, Val_Loss: 1.5883, Val_Acc: 0.5066\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5527 - loss: 1.4273 - val_accuracy: 0.5066 - val_loss: 1.5883\n",
            "Epoch 109/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5535 - loss: 1.4249Epoch 109/200, Time: 20.42s, Loss: 1.4345, Val_Loss: 1.5354, Val_Acc: 0.5174\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5535 - loss: 1.4249 - val_accuracy: 0.5174 - val_loss: 1.5354\n",
            "Epoch 110/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5493 - loss: 1.4254Epoch 110/200, Time: 20.91s, Loss: 1.4302, Val_Loss: 1.5341, Val_Acc: 0.5282\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5494 - loss: 1.4255 - val_accuracy: 0.5282 - val_loss: 1.5341\n",
            "Epoch 111/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5552 - loss: 1.4196Epoch 111/200, Time: 20.25s, Loss: 1.4307, Val_Loss: 1.4935, Val_Acc: 0.5378\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5552 - loss: 1.4197 - val_accuracy: 0.5378 - val_loss: 1.4935\n",
            "Epoch 112/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5536 - loss: 1.4207Epoch 112/200, Time: 20.47s, Loss: 1.4263, Val_Loss: 1.5354, Val_Acc: 0.5140\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.5536 - loss: 1.4208 - val_accuracy: 0.5140 - val_loss: 1.5354\n",
            "Epoch 113/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5529 - loss: 1.4217Epoch 113/200, Time: 11.80s, Loss: 1.4248, Val_Loss: 1.5374, Val_Acc: 0.5228\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5529 - loss: 1.4217 - val_accuracy: 0.5228 - val_loss: 1.5374\n",
            "Epoch 114/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5600 - loss: 1.4027Epoch 114/200, Time: 20.47s, Loss: 1.4178, Val_Loss: 1.4857, Val_Acc: 0.5248\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5600 - loss: 1.4028 - val_accuracy: 0.5248 - val_loss: 1.4857\n",
            "Epoch 115/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5589 - loss: 1.4057Epoch 115/200, Time: 12.05s, Loss: 1.4182, Val_Loss: 1.5202, Val_Acc: 0.5210\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5588 - loss: 1.4057 - val_accuracy: 0.5210 - val_loss: 1.5202\n",
            "Epoch 116/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5619 - loss: 1.3961Epoch 116/200, Time: 20.63s, Loss: 1.4123, Val_Loss: 1.4668, Val_Acc: 0.5476\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5619 - loss: 1.3962 - val_accuracy: 0.5476 - val_loss: 1.4668\n",
            "Epoch 117/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5604 - loss: 1.4017Epoch 117/200, Time: 20.41s, Loss: 1.4092, Val_Loss: 1.5560, Val_Acc: 0.5138\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5604 - loss: 1.4018 - val_accuracy: 0.5138 - val_loss: 1.5560\n",
            "Epoch 118/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5591 - loss: 1.4033Epoch 118/200, Time: 20.49s, Loss: 1.4090, Val_Loss: 1.5047, Val_Acc: 0.5290\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5591 - loss: 1.4034 - val_accuracy: 0.5290 - val_loss: 1.5047\n",
            "Epoch 119/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5569 - loss: 1.4025Epoch 119/200, Time: 11.78s, Loss: 1.4042, Val_Loss: 1.5790, Val_Acc: 0.5122\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5569 - loss: 1.4025 - val_accuracy: 0.5122 - val_loss: 1.5790\n",
            "Epoch 120/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5612 - loss: 1.3942Epoch 120/200, Time: 11.86s, Loss: 1.4044, Val_Loss: 1.4984, Val_Acc: 0.5254\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5612 - loss: 1.3943 - val_accuracy: 0.5254 - val_loss: 1.4984\n",
            "Epoch 121/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5634 - loss: 1.3904Epoch 121/200, Time: 12.14s, Loss: 1.3995, Val_Loss: 1.5261, Val_Acc: 0.5278\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5634 - loss: 1.3904 - val_accuracy: 0.5278 - val_loss: 1.5261\n",
            "Epoch 122/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5640 - loss: 1.3898Epoch 122/200, Time: 11.72s, Loss: 1.3969, Val_Loss: 1.4816, Val_Acc: 0.5404\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5640 - loss: 1.3899 - val_accuracy: 0.5404 - val_loss: 1.4816\n",
            "Epoch 123/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5712 - loss: 1.3761Epoch 123/200, Time: 20.82s, Loss: 1.3920, Val_Loss: 1.4595, Val_Acc: 0.5450\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.5712 - loss: 1.3763 - val_accuracy: 0.5450 - val_loss: 1.4595\n",
            "Epoch 124/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5665 - loss: 1.3802Epoch 124/200, Time: 12.14s, Loss: 1.3886, Val_Loss: 1.4588, Val_Acc: 0.5492\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5665 - loss: 1.3803 - val_accuracy: 0.5492 - val_loss: 1.4588\n",
            "Epoch 125/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5663 - loss: 1.3839Epoch 125/200, Time: 20.55s, Loss: 1.3864, Val_Loss: 1.4580, Val_Acc: 0.5406\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5663 - loss: 1.3840 - val_accuracy: 0.5406 - val_loss: 1.4580\n",
            "Epoch 126/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5700 - loss: 1.3764Epoch 126/200, Time: 12.09s, Loss: 1.3843, Val_Loss: 1.5131, Val_Acc: 0.5236\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5699 - loss: 1.3765 - val_accuracy: 0.5236 - val_loss: 1.5131\n",
            "Epoch 127/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5692 - loss: 1.3793Epoch 127/200, Time: 11.83s, Loss: 1.3809, Val_Loss: 1.5049, Val_Acc: 0.5270\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5692 - loss: 1.3793 - val_accuracy: 0.5270 - val_loss: 1.5049\n",
            "Epoch 128/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5708 - loss: 1.3715Epoch 128/200, Time: 20.51s, Loss: 1.3766, Val_Loss: 1.4579, Val_Acc: 0.5398\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5708 - loss: 1.3715 - val_accuracy: 0.5398 - val_loss: 1.4579\n",
            "Epoch 129/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5695 - loss: 1.3672Epoch 129/200, Time: 11.99s, Loss: 1.3707, Val_Loss: 1.4846, Val_Acc: 0.5454\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5695 - loss: 1.3673 - val_accuracy: 0.5454 - val_loss: 1.4846\n",
            "Epoch 130/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5773 - loss: 1.3481Epoch 130/200, Time: 11.84s, Loss: 1.3688, Val_Loss: 1.4470, Val_Acc: 0.5560\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5772 - loss: 1.3482 - val_accuracy: 0.5560 - val_loss: 1.4470\n",
            "Epoch 131/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5754 - loss: 1.3561Epoch 131/200, Time: 12.16s, Loss: 1.3670, Val_Loss: 1.4704, Val_Acc: 0.5482\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5754 - loss: 1.3561 - val_accuracy: 0.5482 - val_loss: 1.4704\n",
            "Epoch 132/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5797 - loss: 1.3489Epoch 132/200, Time: 12.23s, Loss: 1.3601, Val_Loss: 1.4580, Val_Acc: 0.5478\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5797 - loss: 1.3490 - val_accuracy: 0.5478 - val_loss: 1.4580\n",
            "Epoch 133/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5791 - loss: 1.3508Epoch 133/200, Time: 20.37s, Loss: 1.3603, Val_Loss: 1.4694, Val_Acc: 0.5326\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.5791 - loss: 1.3508 - val_accuracy: 0.5326 - val_loss: 1.4694\n",
            "Epoch 134/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5824 - loss: 1.3458Epoch 134/200, Time: 20.29s, Loss: 1.3532, Val_Loss: 1.4410, Val_Acc: 0.5540\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5824 - loss: 1.3459 - val_accuracy: 0.5540 - val_loss: 1.4410\n",
            "Epoch 135/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5804 - loss: 1.3408Epoch 135/200, Time: 12.15s, Loss: 1.3508, Val_Loss: 1.4458, Val_Acc: 0.5506\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5804 - loss: 1.3408 - val_accuracy: 0.5506 - val_loss: 1.4458\n",
            "Epoch 136/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5883 - loss: 1.3271Epoch 136/200, Time: 11.95s, Loss: 1.3439, Val_Loss: 1.4519, Val_Acc: 0.5406\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5883 - loss: 1.3272 - val_accuracy: 0.5406 - val_loss: 1.4519\n",
            "Epoch 137/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5869 - loss: 1.3194Epoch 137/200, Time: 20.69s, Loss: 1.3380, Val_Loss: 1.4550, Val_Acc: 0.5486\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5869 - loss: 1.3195 - val_accuracy: 0.5486 - val_loss: 1.4550\n",
            "Epoch 138/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5840 - loss: 1.3224Epoch 138/200, Time: 11.89s, Loss: 1.3353, Val_Loss: 1.4371, Val_Acc: 0.5558\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5839 - loss: 1.3224 - val_accuracy: 0.5558 - val_loss: 1.4371\n",
            "Epoch 139/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5855 - loss: 1.3243Epoch 139/200, Time: 20.52s, Loss: 1.3361, Val_Loss: 1.4542, Val_Acc: 0.5456\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.5855 - loss: 1.3244 - val_accuracy: 0.5456 - val_loss: 1.4542\n",
            "Epoch 140/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5867 - loss: 1.3199Epoch 140/200, Time: 12.16s, Loss: 1.3309, Val_Loss: 1.4197, Val_Acc: 0.5604\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.5867 - loss: 1.3199 - val_accuracy: 0.5604 - val_loss: 1.4197\n",
            "Epoch 141/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5939 - loss: 1.2969Epoch 141/200, Time: 12.15s, Loss: 1.3208, Val_Loss: 1.4651, Val_Acc: 0.5492\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.5938 - loss: 1.2971 - val_accuracy: 0.5492 - val_loss: 1.4651\n",
            "Epoch 142/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5926 - loss: 1.3121Epoch 142/200, Time: 12.20s, Loss: 1.3194, Val_Loss: 1.4489, Val_Acc: 0.5538\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.5926 - loss: 1.3122 - val_accuracy: 0.5538 - val_loss: 1.4489\n",
            "Epoch 143/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5946 - loss: 1.3032Epoch 143/200, Time: 20.38s, Loss: 1.3148, Val_Loss: 1.4555, Val_Acc: 0.5466\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.5946 - loss: 1.3033 - val_accuracy: 0.5466 - val_loss: 1.4555\n",
            "Epoch 144/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5977 - loss: 1.2979Epoch 144/200, Time: 20.25s, Loss: 1.3135, Val_Loss: 1.4524, Val_Acc: 0.5498\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.5977 - loss: 1.2980 - val_accuracy: 0.5498 - val_loss: 1.4524\n",
            "Epoch 145/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5986 - loss: 1.2945Epoch 145/200, Time: 20.84s, Loss: 1.3062, Val_Loss: 1.4312, Val_Acc: 0.5652\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.5986 - loss: 1.2946 - val_accuracy: 0.5652 - val_loss: 1.4312\n",
            "Epoch 146/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6024 - loss: 1.2862Epoch 146/200, Time: 11.83s, Loss: 1.3001, Val_Loss: 1.4413, Val_Acc: 0.5544\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6023 - loss: 1.2863 - val_accuracy: 0.5544 - val_loss: 1.4413\n",
            "Epoch 147/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6027 - loss: 1.2867Epoch 147/200, Time: 11.83s, Loss: 1.2971, Val_Loss: 1.4815, Val_Acc: 0.5412\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6027 - loss: 1.2868 - val_accuracy: 0.5412 - val_loss: 1.4815\n",
            "Epoch 148/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6016 - loss: 1.2778Epoch 148/200, Time: 11.80s, Loss: 1.2926, Val_Loss: 1.4143, Val_Acc: 0.5638\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.6016 - loss: 1.2779 - val_accuracy: 0.5638 - val_loss: 1.4143\n",
            "Epoch 149/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6068 - loss: 1.2689Epoch 149/200, Time: 12.04s, Loss: 1.2861, Val_Loss: 1.4294, Val_Acc: 0.5540\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6068 - loss: 1.2690 - val_accuracy: 0.5540 - val_loss: 1.4294\n",
            "Epoch 150/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6082 - loss: 1.2652Epoch 150/200, Time: 11.67s, Loss: 1.2812, Val_Loss: 1.4320, Val_Acc: 0.5600\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6082 - loss: 1.2653 - val_accuracy: 0.5600 - val_loss: 1.4320\n",
            "Epoch 151/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6124 - loss: 1.2576Epoch 151/200, Time: 11.82s, Loss: 1.2773, Val_Loss: 1.4201, Val_Acc: 0.5700\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6123 - loss: 1.2577 - val_accuracy: 0.5700 - val_loss: 1.4201\n",
            "Epoch 152/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6134 - loss: 1.2546Epoch 152/200, Time: 20.52s, Loss: 1.2690, Val_Loss: 1.4392, Val_Acc: 0.5562\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.6134 - loss: 1.2547 - val_accuracy: 0.5562 - val_loss: 1.4392\n",
            "Epoch 153/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6152 - loss: 1.2528Epoch 153/200, Time: 12.07s, Loss: 1.2652, Val_Loss: 1.4191, Val_Acc: 0.5634\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.6152 - loss: 1.2529 - val_accuracy: 0.5634 - val_loss: 1.4191\n",
            "Epoch 154/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6134 - loss: 1.2485Epoch 154/200, Time: 12.29s, Loss: 1.2590, Val_Loss: 1.4202, Val_Acc: 0.5678\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.6134 - loss: 1.2486 - val_accuracy: 0.5678 - val_loss: 1.4202\n",
            "Epoch 155/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6131 - loss: 1.2495Epoch 155/200, Time: 11.89s, Loss: 1.2546, Val_Loss: 1.4112, Val_Acc: 0.5634\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.6131 - loss: 1.2495 - val_accuracy: 0.5634 - val_loss: 1.4112\n",
            "Epoch 156/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6209 - loss: 1.2279Epoch 156/200, Time: 20.37s, Loss: 1.2490, Val_Loss: 1.4154, Val_Acc: 0.5714\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.6209 - loss: 1.2280 - val_accuracy: 0.5714 - val_loss: 1.4154\n",
            "Epoch 157/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6227 - loss: 1.2263Epoch 157/200, Time: 20.68s, Loss: 1.2424, Val_Loss: 1.4375, Val_Acc: 0.5712\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.6226 - loss: 1.2265 - val_accuracy: 0.5712 - val_loss: 1.4375\n",
            "Epoch 158/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6239 - loss: 1.2184Epoch 158/200, Time: 11.84s, Loss: 1.2369, Val_Loss: 1.4073, Val_Acc: 0.5642\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.6238 - loss: 1.2185 - val_accuracy: 0.5642 - val_loss: 1.4073\n",
            "Epoch 159/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6230 - loss: 1.2242Epoch 159/200, Time: 12.59s, Loss: 1.2324, Val_Loss: 1.4134, Val_Acc: 0.5598\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.6230 - loss: 1.2243 - val_accuracy: 0.5598 - val_loss: 1.4134\n",
            "Epoch 160/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6256 - loss: 1.2099Epoch 160/200, Time: 12.37s, Loss: 1.2268, Val_Loss: 1.3846, Val_Acc: 0.5790\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 36ms/step - accuracy: 0.6256 - loss: 1.2100 - val_accuracy: 0.5790 - val_loss: 1.3846\n",
            "Epoch 161/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6323 - loss: 1.1994Epoch 161/200, Time: 20.13s, Loss: 1.2189, Val_Loss: 1.4124, Val_Acc: 0.5748\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.6323 - loss: 1.1994 - val_accuracy: 0.5748 - val_loss: 1.4124\n",
            "Epoch 162/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6299 - loss: 1.2014Epoch 162/200, Time: 11.60s, Loss: 1.2138, Val_Loss: 1.4018, Val_Acc: 0.5756\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6298 - loss: 1.2015 - val_accuracy: 0.5756 - val_loss: 1.4018\n",
            "Epoch 163/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6304 - loss: 1.1994Epoch 163/200, Time: 11.91s, Loss: 1.2071, Val_Loss: 1.4012, Val_Acc: 0.5758\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6304 - loss: 1.1995 - val_accuracy: 0.5758 - val_loss: 1.4012\n",
            "Epoch 164/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6389 - loss: 1.1780Epoch 164/200, Time: 20.82s, Loss: 1.2002, Val_Loss: 1.4063, Val_Acc: 0.5714\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.6389 - loss: 1.1781 - val_accuracy: 0.5714 - val_loss: 1.4063\n",
            "Epoch 165/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6398 - loss: 1.1683Epoch 165/200, Time: 12.12s, Loss: 1.1913, Val_Loss: 1.4005, Val_Acc: 0.5714\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.6397 - loss: 1.1685 - val_accuracy: 0.5714 - val_loss: 1.4005\n",
            "Epoch 166/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6423 - loss: 1.1657Epoch 166/200, Time: 20.35s, Loss: 1.1838, Val_Loss: 1.3997, Val_Acc: 0.5708\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.6422 - loss: 1.1658 - val_accuracy: 0.5708 - val_loss: 1.3997\n",
            "Epoch 167/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6417 - loss: 1.1641Epoch 167/200, Time: 20.31s, Loss: 1.1805, Val_Loss: 1.3812, Val_Acc: 0.5780\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.6417 - loss: 1.1642 - val_accuracy: 0.5780 - val_loss: 1.3812\n",
            "Epoch 168/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6452 - loss: 1.1606Epoch 168/200, Time: 12.20s, Loss: 1.1701, Val_Loss: 1.3902, Val_Acc: 0.5798\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6451 - loss: 1.1607 - val_accuracy: 0.5798 - val_loss: 1.3902\n",
            "Epoch 169/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6478 - loss: 1.1452Epoch 169/200, Time: 11.99s, Loss: 1.1647, Val_Loss: 1.3972, Val_Acc: 0.5774\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.6477 - loss: 1.1454 - val_accuracy: 0.5774 - val_loss: 1.3972\n",
            "Epoch 170/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6518 - loss: 1.1470Epoch 170/200, Time: 20.41s, Loss: 1.1572, Val_Loss: 1.4049, Val_Acc: 0.5694\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.6517 - loss: 1.1471 - val_accuracy: 0.5694 - val_loss: 1.4049\n",
            "Epoch 171/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6494 - loss: 1.1447Epoch 171/200, Time: 20.55s, Loss: 1.1493, Val_Loss: 1.4007, Val_Acc: 0.5786\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.6494 - loss: 1.1447 - val_accuracy: 0.5786 - val_loss: 1.4007\n",
            "Epoch 172/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6562 - loss: 1.1286Epoch 172/200, Time: 20.75s, Loss: 1.1408, Val_Loss: 1.3918, Val_Acc: 0.5780\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.6561 - loss: 1.1287 - val_accuracy: 0.5780 - val_loss: 1.3918\n",
            "Epoch 173/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6590 - loss: 1.1238Epoch 173/200, Time: 20.22s, Loss: 1.1352, Val_Loss: 1.3952, Val_Acc: 0.5856\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.6589 - loss: 1.1239 - val_accuracy: 0.5856 - val_loss: 1.3952\n",
            "Epoch 174/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6588 - loss: 1.1168Epoch 174/200, Time: 12.24s, Loss: 1.1256, Val_Loss: 1.3941, Val_Acc: 0.5820\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.6588 - loss: 1.1168 - val_accuracy: 0.5820 - val_loss: 1.3941\n",
            "Epoch 175/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6642 - loss: 1.1029Epoch 175/200, Time: 20.44s, Loss: 1.1176, Val_Loss: 1.3953, Val_Acc: 0.5788\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.6642 - loss: 1.1030 - val_accuracy: 0.5788 - val_loss: 1.3953\n",
            "Epoch 176/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6639 - loss: 1.1022Epoch 176/200, Time: 20.58s, Loss: 1.1104, Val_Loss: 1.3984, Val_Acc: 0.5874\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.6639 - loss: 1.1023 - val_accuracy: 0.5874 - val_loss: 1.3984\n",
            "Epoch 177/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6673 - loss: 1.0978Epoch 177/200, Time: 20.28s, Loss: 1.1039, Val_Loss: 1.4035, Val_Acc: 0.5742\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.6673 - loss: 1.0978 - val_accuracy: 0.5742 - val_loss: 1.4035\n",
            "Epoch 178/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6714 - loss: 1.0825Epoch 178/200, Time: 12.01s, Loss: 1.0935, Val_Loss: 1.4073, Val_Acc: 0.5758\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.6714 - loss: 1.0826 - val_accuracy: 0.5758 - val_loss: 1.4073\n",
            "Epoch 179/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6679 - loss: 1.0794Epoch 179/200, Time: 20.39s, Loss: 1.0866, Val_Loss: 1.3841, Val_Acc: 0.5864\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.6679 - loss: 1.0794 - val_accuracy: 0.5864 - val_loss: 1.3841\n",
            "Epoch 180/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6752 - loss: 1.0731Epoch 180/200, Time: 11.88s, Loss: 1.0771, Val_Loss: 1.3957, Val_Acc: 0.5796\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6752 - loss: 1.0731 - val_accuracy: 0.5796 - val_loss: 1.3957\n",
            "Epoch 181/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6797 - loss: 1.0601Epoch 181/200, Time: 20.57s, Loss: 1.0709, Val_Loss: 1.3955, Val_Acc: 0.5876\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.6797 - loss: 1.0601 - val_accuracy: 0.5876 - val_loss: 1.3955\n",
            "Epoch 182/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6816 - loss: 1.0587Epoch 182/200, Time: 20.72s, Loss: 1.0607, Val_Loss: 1.3904, Val_Acc: 0.5866\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.6816 - loss: 1.0588 - val_accuracy: 0.5866 - val_loss: 1.3904\n",
            "Epoch 183/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6856 - loss: 1.0379Epoch 183/200, Time: 20.25s, Loss: 1.0530, Val_Loss: 1.4003, Val_Acc: 0.5788\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.6855 - loss: 1.0380 - val_accuracy: 0.5788 - val_loss: 1.4003\n",
            "Epoch 184/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6878 - loss: 1.0411Epoch 184/200, Time: 11.81s, Loss: 1.0474, Val_Loss: 1.3926, Val_Acc: 0.5864\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6878 - loss: 1.0412 - val_accuracy: 0.5864 - val_loss: 1.3926\n",
            "Epoch 185/200\n",
            "\u001b[1m350/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6924 - loss: 1.0258Epoch 185/200, Time: 11.91s, Loss: 1.0394, Val_Loss: 1.4011, Val_Acc: 0.5862\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.6923 - loss: 1.0259 - val_accuracy: 0.5862 - val_loss: 1.4011\n",
            "Epoch 186/200\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6950 - loss: 1.0202Epoch 186/200, Time: 11.85s, Loss: 1.0269, Val_Loss: 1.3876, Val_Acc: 0.5874\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.6950 - loss: 1.0203 - val_accuracy: 0.5874 - val_loss: 1.3876\n",
            "Epoch 187/200\n",
            "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7000 - loss: 1.0116Epoch 187/200, Time: 20.86s, Loss: 1.0208, Val_Loss: 1.3973, Val_Acc: 0.5858\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.7000 - loss: 1.0116 - val_accuracy: 0.5858 - val_loss: 1.3973\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'is_floating'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bffc67d1c9d1>\u001b[0m in \u001b[0;36m<cell line: 233>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;31m# Evaluate on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0msparsity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiming_cb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bffc67d1c9d1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;31m# Evaluate on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0msparsity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiming_cb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_floating'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "####################################\n",
        "# Hyperparameters\n",
        "####################################\n",
        "batch_size = 128\n",
        "lr = 0.01\n",
        "epochs = 2  # reduced epochs for demonstration\n",
        "alpha = 0.1\n",
        "equilibrium_iters = 5\n",
        "\n",
        "####################################\n",
        "# Data Loading\n",
        "####################################\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "####################################\n",
        "# Model Definition\n",
        "####################################\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SmallCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((4,4)),\n",
        "        )\n",
        "        self.fc = nn.Linear(64*4*4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.features(x)\n",
        "        f_flat = f.view(f.size(0), -1)\n",
        "        # The fully connected layer gives an initial guess if needed, not strictly required\n",
        "        out = self.fc(f_flat)\n",
        "        return f_flat, out\n",
        "\n",
        "model = SmallCNN().cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "####################################\n",
        "# Energy Function\n",
        "####################################\n",
        "def energy(x, target, alpha, f):\n",
        "    # E(x) = CrossEntropy(x, target) + alpha * ||x||^2\n",
        "    ce = F.cross_entropy(x, target)\n",
        "    reg = (x**2).mean()\n",
        "    return ce + alpha * reg\n",
        "\n",
        "####################################\n",
        "# Solve Equilibrium\n",
        "####################################\n",
        "@torch.enable_grad()\n",
        "def solve_equilibrium(f, target, alpha):\n",
        "    # We initialize x at zero with requires_grad=True\n",
        "    x = torch.zeros(target.size(0), 10, device=target.device, requires_grad=True)\n",
        "\n",
        "    # Perform gradient descent steps on x\n",
        "    # We need create_graph=True to allow differentiation w.r.t. model parameters later\n",
        "    for i in range(equilibrium_iters):\n",
        "        E = energy(x, target, alpha, f)\n",
        "        grad_x = torch.autograd.grad(E, x, retain_graph=True, create_graph=True)[0]\n",
        "        x = x - 0.1 * grad_x\n",
        "        # x here is a new tensor resulting from an operation, but still has a grad_fn\n",
        "        # No need to manually set requires_grad=True; x will have grad_fn attached.\n",
        "\n",
        "    return x\n",
        "\n",
        "####################################\n",
        "# Training Loop\n",
        "####################################\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        f, _ = model(inputs)\n",
        "        x_star = solve_equilibrium(f, targets, alpha)\n",
        "\n",
        "        # Now we compute E_final and backprop into model parameters\n",
        "        E_final = energy(x_star, targets, alpha, f)\n",
        "        E_final.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            f, _ = model(inputs)\n",
        "            # Even at test time, we solve for equilibrium\n",
        "            x_star = solve_equilibrium(f, targets, alpha)\n",
        "            pred = x_star.argmax(dim=1)\n",
        "            correct += pred.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "    acc = 100. * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Test Accuracy: {acc:.2f}%\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTG9YFLDpfcl",
        "outputId": "fcf9f606-2ae1-43f9-ac0a-90145c484ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/2, Test Accuracy: 100.00%\n",
            "Epoch 2/2, Test Accuracy: 100.00%\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "####################################\n",
        "# Hyperparameters\n",
        "####################################\n",
        "batch_size = 128\n",
        "lr = 0.01\n",
        "epochs = 10\n",
        "alpha = 0.1\n",
        "equilibrium_iters = 5\n",
        "\n",
        "####################################\n",
        "# Data Loading\n",
        "####################################\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "####################################\n",
        "# Model Definition\n",
        "####################################\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SmallCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((4,4)),\n",
        "        )\n",
        "        self.fc = nn.Linear(64*4*4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.features(x)\n",
        "        f_flat = f.view(f.size(0), -1)\n",
        "        out = self.fc(f_flat)\n",
        "        return f_flat, out\n",
        "\n",
        "model = SmallCNN().cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "####################################\n",
        "# Energy Function\n",
        "####################################\n",
        "def energy(x, target, alpha, f):\n",
        "    # E(x) = CrossEntropy(x, target) + alpha * ||x||^2\n",
        "    ce = F.cross_entropy(x, target)\n",
        "    reg = (x**2).mean()\n",
        "    return ce + alpha * reg\n",
        "\n",
        "####################################\n",
        "# Solve Equilibrium (Train time)\n",
        "####################################\n",
        "@torch.enable_grad()\n",
        "def solve_equilibrium_train(f, target, alpha):\n",
        "    # Initialize x with requires_grad=True\n",
        "    x = torch.zeros(target.size(0), 10, device=target.device, requires_grad=True)\n",
        "    for i in range(equilibrium_iters):\n",
        "        E = energy(x, target, alpha, f)\n",
        "        grad_x = torch.autograd.grad(E, x, retain_graph=True, create_graph=True)[0]\n",
        "        x = x - 0.1 * grad_x\n",
        "        # No need to manually set requires_grad here; x already has grad_fn.\n",
        "    return x\n",
        "\n",
        "####################################\n",
        "# Training Loop\n",
        "####################################\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        f, _ = model(inputs)\n",
        "        x_star = solve_equilibrium_train(f, targets, alpha)\n",
        "\n",
        "        # Compute final energy and backprop w.r.t. model parameters\n",
        "        E_final = energy(x_star, targets, alpha, f)\n",
        "        E_final.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            f, out = model(inputs)\n",
        "            # At test time, we do not solve for equilibrium with targets.\n",
        "            # We just use model's direct output:\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += pred.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "    acc = 100. * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Test Accuracy: {acc:.2f}%\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF7ThQBGrm05",
        "outputId": "fde2a46c-ea94-4fd3-930d-1986ba2264bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/10, Test Accuracy: 10.36%\n",
            "Epoch 2/10, Test Accuracy: 10.36%\n",
            "Epoch 3/10, Test Accuracy: 10.36%\n",
            "Epoch 4/10, Test Accuracy: 10.36%\n",
            "Epoch 5/10, Test Accuracy: 10.36%\n",
            "Epoch 6/10, Test Accuracy: 10.36%\n",
            "Epoch 7/10, Test Accuracy: 10.36%\n",
            "Epoch 8/10, Test Accuracy: 10.36%\n",
            "Epoch 9/10, Test Accuracy: 10.36%\n",
            "Epoch 10/10, Test Accuracy: 10.36%\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "####################################\n",
        "# Hyperparameters\n",
        "####################################\n",
        "batch_size = 128\n",
        "lr = 0.01\n",
        "epochs = 10\n",
        "alpha = 0.01\n",
        "equilibrium_iters = 10\n",
        "\n",
        "####################################\n",
        "# Data Loading\n",
        "####################################\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "####################################\n",
        "# Model Definition\n",
        "####################################\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SmallCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((4,4)),\n",
        "        )\n",
        "        self.fc = nn.Linear(64*4*4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.features(x)\n",
        "        f_flat = f.view(f.size(0), -1)\n",
        "        out = self.fc(f_flat)\n",
        "        return f_flat, out\n",
        "\n",
        "model = SmallCNN().cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "####################################\n",
        "# Energy Function\n",
        "####################################\n",
        "def energy(x, out, target, alpha):\n",
        "    # Modified energy:\n",
        "    # E(x) = CE(x, target) + alpha * ||x - out||^2\n",
        "    ce = F.cross_entropy(x, target)\n",
        "    diff = x - out\n",
        "    reg = (diff**2).mean()\n",
        "    return ce + alpha * reg\n",
        "\n",
        "####################################\n",
        "# Solve Equilibrium (Train time)\n",
        "####################################\n",
        "@torch.enable_grad()\n",
        "def solve_equilibrium_train(out, target, alpha):\n",
        "    # Initialize x from model output out\n",
        "    x = out.clone().detach().requires_grad_(True)\n",
        "\n",
        "    for i in range(equilibrium_iters):\n",
        "        E = energy(x, out, target, alpha)\n",
        "        grad_x = torch.autograd.grad(E, x, retain_graph=True, create_graph=True)[0]\n",
        "        x = x - 0.1 * grad_x\n",
        "        # No need to manually set requires_grad again; x from operation will have grad_fn.\n",
        "        x.requires_grad_(True)\n",
        "    return x\n",
        "\n",
        "####################################\n",
        "# Training Loop\n",
        "####################################\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        f, out = model(inputs)\n",
        "        x_star = solve_equilibrium_train(out, targets, alpha)\n",
        "\n",
        "        # Compute final energy and backprop\n",
        "        E_final = energy(x_star, out, targets, alpha)\n",
        "        E_final.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate on test set without equilibrium, just use model outputs\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            f, out = model(inputs)\n",
        "            # Direct model output as prediction\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += pred.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "    acc = 100. * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Test Accuracy: {acc:.2f}%\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9V3n0FxsiJO",
        "outputId": "09409f3c-5158-4178-83de-625a360f3d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/10, Test Accuracy: 10.85%\n",
            "Epoch 2/10, Test Accuracy: 10.85%\n",
            "Epoch 3/10, Test Accuracy: 10.87%\n",
            "Epoch 4/10, Test Accuracy: 10.88%\n",
            "Epoch 5/10, Test Accuracy: 10.87%\n",
            "Epoch 6/10, Test Accuracy: 10.86%\n",
            "Epoch 7/10, Test Accuracy: 10.86%\n",
            "Epoch 8/10, Test Accuracy: 10.85%\n",
            "Epoch 9/10, Test Accuracy: 10.84%\n",
            "Epoch 10/10, Test Accuracy: 10.84%\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "####################################\n",
        "# Hyperparameters\n",
        "####################################\n",
        "batch_size = 128\n",
        "lr = 0.1\n",
        "epochs = 50\n",
        "alpha = 0.001\n",
        "equilibrium_iters = 20\n",
        "equilibrium_step_size = 0.1  # step size for x updates\n",
        "\n",
        "####################################\n",
        "# Data Loading\n",
        "####################################\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "####################################\n",
        "# Model Definition\n",
        "####################################\n",
        "# Use ResNet-18 and adjust final layer for CIFAR-10\n",
        "model = resnet18(weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "model = model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.1)  # Reduce LR every 20 epochs\n",
        "\n",
        "####################################\n",
        "# Energy Function\n",
        "####################################\n",
        "def energy(x, out, target, alpha):\n",
        "    # E(x) = CE(x, target) + alpha * ||x - out||^2\n",
        "    ce = F.cross_entropy(x, target)\n",
        "    diff = x - out\n",
        "    reg = (diff**2).mean()\n",
        "    return ce + alpha * reg\n",
        "\n",
        "####################################\n",
        "# Solve Equilibrium (Train time)\n",
        "####################################\n",
        "@torch.enable_grad()\n",
        "def solve_equilibrium_train(out, target, alpha):\n",
        "    # Initialize x from model output out\n",
        "    x = out.clone().detach().requires_grad_(True)\n",
        "\n",
        "    for i in range(equilibrium_iters):\n",
        "        E = energy(x, out, target, alpha)\n",
        "        grad_x = torch.autograd.grad(E, x, retain_graph=True, create_graph=True)[0]\n",
        "        x = x - equilibrium_step_size * grad_x\n",
        "        x.requires_grad_(True)\n",
        "    return x\n",
        "\n",
        "####################################\n",
        "# Training and Evaluation\n",
        "####################################\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(inputs)\n",
        "        # Equilibrium solving for training\n",
        "        x_star = solve_equilibrium_train(out, targets, alpha)\n",
        "\n",
        "        # Compute final energy and backprop\n",
        "        E_final = energy(x_star, out, targets, alpha)\n",
        "        E_final.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate on test set (no equilibrium, just direct output)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            out = model(inputs)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += pred.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "    acc = 100. * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Test Accuracy: {acc:.2f}%\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S69qQMYpusKG",
        "outputId": "172ac0ed-1b97-4d75-e197-30f0b045a089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/50, Test Accuracy: 9.89%\n",
            "Epoch 2/50, Test Accuracy: 10.07%\n",
            "Epoch 3/50, Test Accuracy: 10.31%\n",
            "Epoch 4/50, Test Accuracy: 10.40%\n",
            "Epoch 5/50, Test Accuracy: 10.62%\n",
            "Epoch 6/50, Test Accuracy: 10.97%\n",
            "Epoch 7/50, Test Accuracy: 11.18%\n",
            "Epoch 8/50, Test Accuracy: 11.47%\n",
            "Epoch 9/50, Test Accuracy: 11.66%\n",
            "Epoch 10/50, Test Accuracy: 11.69%\n",
            "Epoch 11/50, Test Accuracy: 11.68%\n",
            "Epoch 12/50, Test Accuracy: 11.35%\n",
            "Epoch 13/50, Test Accuracy: 10.27%\n",
            "Epoch 14/50, Test Accuracy: 9.88%\n",
            "Epoch 15/50, Test Accuracy: 10.32%\n",
            "Epoch 16/50, Test Accuracy: 9.99%\n",
            "Epoch 17/50, Test Accuracy: 10.00%\n",
            "Epoch 18/50, Test Accuracy: 10.00%\n",
            "Epoch 19/50, Test Accuracy: 10.00%\n",
            "Epoch 20/50, Test Accuracy: 10.00%\n",
            "Epoch 21/50, Test Accuracy: 10.00%\n",
            "Epoch 22/50, Test Accuracy: 10.00%\n",
            "Epoch 23/50, Test Accuracy: 10.00%\n",
            "Epoch 24/50, Test Accuracy: 10.00%\n",
            "Epoch 25/50, Test Accuracy: 10.00%\n",
            "Epoch 26/50, Test Accuracy: 10.00%\n",
            "Epoch 27/50, Test Accuracy: 10.00%\n",
            "Epoch 28/50, Test Accuracy: 10.00%\n",
            "Epoch 29/50, Test Accuracy: 10.00%\n",
            "Epoch 30/50, Test Accuracy: 10.00%\n",
            "Epoch 31/50, Test Accuracy: 10.00%\n",
            "Epoch 32/50, Test Accuracy: 10.00%\n",
            "Epoch 33/50, Test Accuracy: 10.00%\n",
            "Epoch 34/50, Test Accuracy: 10.00%\n",
            "Epoch 35/50, Test Accuracy: 10.00%\n",
            "Epoch 36/50, Test Accuracy: 10.00%\n",
            "Epoch 37/50, Test Accuracy: 10.00%\n",
            "Epoch 38/50, Test Accuracy: 10.00%\n",
            "Epoch 39/50, Test Accuracy: 10.00%\n",
            "Epoch 40/50, Test Accuracy: 10.00%\n",
            "Epoch 41/50, Test Accuracy: 10.00%\n",
            "Epoch 42/50, Test Accuracy: 10.00%\n",
            "Epoch 43/50, Test Accuracy: 10.00%\n",
            "Epoch 44/50, Test Accuracy: 10.00%\n",
            "Epoch 45/50, Test Accuracy: 10.00%\n",
            "Epoch 46/50, Test Accuracy: 10.00%\n",
            "Epoch 47/50, Test Accuracy: 10.00%\n",
            "Epoch 48/50, Test Accuracy: 10.00%\n",
            "Epoch 49/50, Test Accuracy: 10.00%\n",
            "Epoch 50/50, Test Accuracy: 10.00%\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K2lawsAIusL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}